https://www.linuxtechi.com/setup-glusterfs-storage-on-centos-7-rhel-7/
https://wiki.centos.org/HowTos/GlusterFSonCentOS

Step 1: Modify host file

On all servers

vi /etc/hosts

172.16.8.31 glusterserver1
172.16.8.32 glusterserver2
172.16.8.33 glusterserver3
172.16.8.34 glusterserver4

Step 2: Install Gluster server on all servers

~]# yum install centos-release-gluster -y
~]# yum install epel-release -y
~]# yum install glusterfs-server -y

Enable Server

~]# systemctl start glusterd
~]# systemctl enable glusterd

Step 3: Enable required ports on all servers

~]# sudo systemctl enable firewalld
~]# reboot
~]# firewall-cmd --zone=public --add-port=24007-24008/tcp --permanent
~]# firewall-cmd --zone=public --add-port=24009/tcp --permanent
~]# firewall-cmd --zone=public --add-service=nfs --add-service=samba --add-service=samba-client --permanent
~]# firewall-cmd --zone=public --add-port=111/tcp --add-port=139/tcp --add-port=445/tcp --add-port=965/tcp --add-port=2049/tcp --add-port=38465-38469/tcp --add-port=631/tcp --add-port=111/udp --add-port=963/udp --add-port=49152-49251/tcp --permanent
~]# firewall-cmd --reload


Step 4:

First Create a storage pool for only two server. The other two servers will be use setup Distribute Replicate Volumes 

Create Storage Pool

[root@glusterserver1 adeelshafqat]# gluster peer probe glusterserver2
peer probe: success. 

[root@glusterserver1 adeelshafqat]# gluster peer status


Step 5: Create Data Brick

Make sure a device is avilable. In my case, I have attached a 30 gb disk (dev/sdc) 

[root@glusterserver2 adeelshafqat]# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
fd0      2:0    1    4K  0 disk 
sda      8:0    0   30G  0 disk 
├─sda1   8:1    0  500M  0 part /boot
└─sda2   8:2    0 29.5G  0 part /
sdb      8:16   0   16G  0 disk 
└─sdb1   8:17   0   16G  0 part /mnt/resource
sdc      8:32   0   32G  0 disk 
sr0     11:0    1 1024M  0 rom  

[root@glusterserver2 adeelshafqat]# pvcreate /dev/sdc
  Physical volume "/dev/sdc" successfully created.

[root@glusterserver2 adeelshafqat]# vgcreate vg_gluster /dev/sdc
  Volume group "vg_gluster" successfully created

[root@glusterserver2 adeelshafqat]# lvcreate -L 25G -n brick2 vg_gluster
  Logical volume "brick2" created.

[root@glusterserver2 adeelshafqat]# lsblk
NAME                MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
fd0                   2:0    1    4K  0 disk 
sda                   8:0    0   30G  0 disk 
├─sda1                8:1    0  500M  0 part /boot
└─sda2                8:2    0 29.5G  0 part /
sdb                   8:16   0   16G  0 disk 
└─sdb1                8:17   0   16G  0 part /mnt/resource
sdc                   8:32   0   32G  0 disk 
└─vg_gluster-brick2 253:0    0   25G  0 lvm  
sr0                  11:0    1 1024M  0 rom  
[root@glusterserver2 adeelshafqat]# 

Setup XFS file systems:

[root@glusterserver2 adeelshafqat]# mkfs.xfs /dev/vg_gluster/brick2
meta-data=/dev/vg_gluster/brick2 isize=512    agcount=4, agsize=1638400 blks
         =                       sectsz=4096  attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=6553600, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=3200, version=2
         =                       sectsz=4096  sunit=1 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0


[root@glusterserver2 adeelshafqat]# mkdir -p /bricks/brick2

[root@glusterserver2 adeelshafqat]# mount /dev/vg_gluster/brick2 /bricks/brick2


Extend the /etc/fstab:
[root@glusterserver2 adeelshafqat]# vi /etc/fstab
/dev/vg_gluster/brick2  /bricks/brick2    xfs     defaults    0 0

[root@glusterserver2 adeelshafqat]# lsblk
NAME                MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
fd0                   2:0    1    4K  0 disk 
sda                   8:0    0   30G  0 disk 
├─sda1                8:1    0  500M  0 part /boot
└─sda2                8:2    0 29.5G  0 part /
sdb                   8:16   0   16G  0 disk 
└─sdb1                8:17   0   16G  0 part /mnt/resource
sdc                   8:32   0   32G  0 disk 
└─vg_gluster-brick2 253:0    0   25G  0 lvm  /bricks/brick2
sr0                  11:0    1 1024M  0 rom 

create a directory with brick under the mount point

mkdir /bricks/brick2/brick

Step 6: Create distributed volume using below gluster command on Server 1

[root@glusterserver1 adeelshafqat]# gluster volume create distvol glusterserver1:/bricks/brick1/brick glusterserver2:/bricks/brick2/brick 
volume create: distvol: success: please start the volume to access data


[root@glusterserver1 adeelshafqat]# gluster volume info distvol
 
Volume Name: distvol
Type: Distribute
Volume ID: dab18a9a-ee45-4651-9562-dede3da9e152
Status: Created
Snapshot Count: 0
Number of Bricks: 2
Transport-type: tcp
Bricks:
Brick1: glusterserver1:/bricks/brick1/brick
Brick2: glusterserver2:/bricks/brick2/brick
Options Reconfigured:
transport.address-family: inet
nfs.disable: on

Step 8: Start Volume

[root@glusterserver2 brick]# gluster volume start distvol
volume start: distvol: success

gluster volume info

Status of gluster servers

gluster peer status


Client Installation

[root@glusterserver4 adeelshafqat]# yum install glusterfs-fuse -y
Loaded plugins: fastestmirror, langpacks
Loading mirror speeds from cached hostfile
 * centos-gluster6: mirrors.vooservers.com
 * epel: mirrors.coreix.net
Package glusterfs-fuse-6.4-1.el7.x86_64 already installed and latest version
Nothing to do

[root@glusterserver4 adeelshafqat]# mkdir /mnt/distvol

[root@glusterserver4 glusterfs]# mount -t glusterfs glusterserver1:/distvol /mnt/distvol/


Log files are stored in /var/log/glusterfs/ folder


Replicate Volume

gluster volume create replicavol replica 2 glusterserver3:/bricks/brick3/brick glusterserver4:/bricks/brick4/brick

[root@glusterserver3 adeelshafqat]# gluster volume info replicavol
 
Volume Name: replicavol
Type: Replicate
Volume ID: 764fa885-09c4-46cc-bb15-9893f239319f
Status: Created
Snapshot Count: 0
Number of Bricks: 1 x 2 = 2
Transport-type: tcp
Bricks:
Brick1: glusterserver3:/bricks/brick3/brick
Brick2: glusterserver4:/bricks/brick4/brick
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off

[root@glusterserver3 adeelshafqat]# gluster volume start replicavol
volume start: replicavol: success


[root@glusterserver4 brick]# df -Th
Filesystem                    Type            Size  Used Avail Use% Mounted on
/dev/sda2                     xfs              30G  1.4G   29G   5% /
devtmpfs                      devtmpfs        2.0G     0  2.0G   0% /dev
tmpfs                         tmpfs           2.0G     0  2.0G   0% /dev/shm
tmpfs                         tmpfs           2.0G  9.0M  2.0G   1% /run
tmpfs                         tmpfs           2.0G     0  2.0G   0% /sys/fs/cgroup
/dev/sda1                     xfs             497M   81M  417M  17% /boot
/dev/sdb1                     ext4            7.8G   36M  7.3G   1% /mnt/resource
tmpfs                         tmpfs           394M     0  394M   0% /run/user/1000
/dev/mapper/vg_gluster-brick4 xfs              25G   33M   25G   1% /bricks/brick4
glusterserver3:/replicavol    fuse.glusterfs   25G  289M   25G   2% /mnt/replicavol

Distributed Replica Volume

[root@glusterserver1 mnt]# gluster peer probe glusterserver3
peer probe: success. 
[root@glusterserver1 mnt]# gluster peer probe glusterserver4
peer probe: success. 

[root@glusterserver1 mnt]# gluster volume create dist-rep-vol replica 2 glusterserver1:/bricks/brick1/brick glusterserver2:/bricks/brick2/brick  glusterserver3:/bricks/brick3/brick glusterserver4:/bricks/brick4/brick
Replica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.
Do you still want to continue?
 (y/n) y
volume create: dist-rep-vol: success: please start the volume to access data

[root@glusterserver1 mnt]# gluster volume info
 
Volume Name: dist-rep-vol
Type: Distributed-Replicate
Volume ID: 27c7cd90-b0ae-4340-bd84-2ca8eaccda4b
Status: Created
Snapshot Count: 0
Number of Bricks: 2 x 2 = 4
Transport-type: tcp
Bricks:
Brick1: glusterserver1:/bricks/brick1/brick
Brick2: glusterserver2:/bricks/brick2/brick
Brick3: glusterserver3:/bricks/brick3/brick
Brick4: glusterserver4:/bricks/brick4/brick
Options Reconfigured:
transport.address-family: inet
nfs.disable: on
performance.client-io-threads: off
[root@glusterserver1 mnt]# 








[root@glusterserver3 /]# gluster volume stop replicavol
Stopping volume will make its data inaccessible. Do you want to continue? (y/n) y
volume stop: replicavol: success
[root@glusterserver3 /]# gluster volume delete replicavol
Deleting volume will erase all information about the volume. Do you want to continue? (y/n) y
volume delete: replicavol: success
[root@glusterserver3 /]# 


[root@glusterserver3 /]# gluster peer detach glusterserver4
All clients mounted through the peer which is getting detached need to be remounted using one of the other active peers in the trusted storage pool to ensure client gets notification on any changes done on the gluster configuration and if the same has been done do you want to proceed? (y/n) y
peer detach: success


Client
[root@glusterserver4 mnt]# mkdir /mnt/distreplicatevol

[root@glusterserver4 mnt]# mount -t glusterfs glusterserver1:/dist-rep-vol /mnt/distreplicatevol/
Mount failed. Please check the log file for more details.

[root@glusterserver4 mnt]# cd /var/log/glusterfs/

[root@glusterserver4 glusterfs]# gluster volume status
Volume dist-rep-vol is not started

[root@glusterserver1 mnt]# gluster volume start dist-rep-vol
volume start: dist-rep-vol: success
[root@glusterserver1 mnt]# 

[root@glusterserver4 glusterfs]# mount -t glusterfs glusterserver1:/dist-rep-vol /mnt/distreplicatevol/


Exceptions


is already part of a volume

[root@glusterserver1 mnt]#  gluster volume create dist-rep-vol replica 2 glusterserver1:/bricks/brick1/brick glusterserver2:/bricks/brick2/brick  glusterserver3:/bricks/brick3/brick glusterserver4:/bricks/brick4/brick

volume create: dist-rep-vol: failed: /bricks/brick1/brick is already part of a volume
[root@glusterserver1 mnt]# gluster volume info
No volumes present

Solution

Run this on all nodes, change brick name accordingly

setfattr -x trusted.glusterfs.volume-id /bricks/brick1/brick
setfattr -x trusted.gfid /bricks/brick1/brick
rm -rf /bricks/brick1/brick/.glusterfs

Issue:

While using Openshift if you see this error

the following error information was pulled from the glusterfs log to help diagnose this issue: 
[2019-08-04 03:03:57.028565] E [MSGID: 100026] [glusterfsd.c:2351:glusterfs_process_volfp] 0-: failed to construct the graph

That means Gluster client and server has different version.

Go to one of the Gluster server and run this command

gluster volume set $volname ctime off
For eg: gluster volume set dist-rep-vol ctime off

Issue:

If your pod can list file but not create files in Gluster server and giving permission error

Go to the master node

sh-4.2$ id root
uid=0(root) gid=0(root) groups=0(root)

oc edit ns my-project

change id value, allow ids from 0

openshift.io/sa.scc.uid-range: 0/10000

Restart application which is using pv (not openshift cluster)


